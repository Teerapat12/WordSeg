{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "import sys\n",
    "sys.path.insert(0, '../script/')\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import preprocess\n",
    "X,Y = preprocess.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 22\n",
       " 19\n",
       " 41\n",
       " 44\n",
       " 25\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Finished cleaning\")\n",
    "def charToNumber(c):\n",
    "    n = ord(c)\n",
    "    if(n>=3585 and n<=3673):\n",
    "        return n-3585\n",
    "    else:\n",
    "        return 89 #For Other\n",
    "\n",
    "def prepare_sequence(sentence):\n",
    "    idxs = [charToNumber(c) for c in sentence]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "prepare_sequence(\"ทดสอบ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initiate parameters\n",
    "emb_size = 32\n",
    "nhidden = 300\n",
    "nlayers = 5\n",
    "dropout = 0.5\n",
    "rnn_type = 'LSTM'\n",
    "n_char = 90\n",
    "clip = 0.25\n",
    "nout = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers,char_size, tagset_size):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(char_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2,num_layers = n_layers,bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers*2, 1, self.hidden_dim//2)),\n",
    "                autograd.Variable(torch.zeros(self.n_layers*2, 1, self.hidden_dim//2)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.softmax(tag_space)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTMTagger(emb_size,nhidden,nlayers,n_char, nout)\n",
    "loss_function = nn.BCELoss()\n",
    "#loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epochs 0...\n",
      "(21589,)\n",
      "(21589,)\n",
      "19ss:  19.0654\n",
      "20\n",
      "+++++++++++++++++++\n",
      "00000000000000000001\n",
      "Skipping\n",
      "วลา|มี|ปัญหา|เกี่ยว|กับ|การ|ปฏิรูป|การ|เมือง|ขึ้น|มา|ครา|ใด| |ก็|จะ|มี|ผู้|ที่|เป็น|นัก|กฎหมาย|มหาชน|และ|รวม|ถึง|ผู้|ที่|เข้าใจ|ว\n",
      "Pred: |ว|ล|า|ม|ี|ป|ั|ญ|ห|า|เ|ก|ี|่|ย|ว|ก|ั|บ|ก|า|ร|ป|ฏ|ิ|ร|ู|ป|ก|า|ร|เ|ม|ื|อ|ง|ข|ึ|้|น|ม|า|ค|ร|า|ใ|ด| |ก|็|จ|ะ|ม|ี|ผ|ู|้|ท|ี|่|เ|ป|็|น|น|ั|ก|ก|ฎ|ห|ม|า|ย|ม|ห|า|ช|น|แ|ล|ะ|ร|ว|ม|ถ|ึ|ง|ผ|ู|้|ท|ี|่|เ|ข|้|า|ใ|จ|ว\n",
      "|แต่|ผู้|ผูกขาด|ความ|รุนแรง|ก็|ไม่|สามารถ|จำกัด|วง|มิ|ให้|ความ|รุนแรง|ระบาด|ไป|ยัง|ส่วน|อื่น|ๆ| |ของ|สังคม|ได้| |ฉะนั้น|สังคม|จึ\n",
      "Pred: |แ|ต|่|ผ|ู|้|ผ|ู|ก|ข|า|ด|ค|ว|า|ม|ร|ุ|น|แ|ร|ง|ก|็|ไ|ม|่|ส|า|ม|า|ร|ถ|จ|ำ|ก|ั|ด|ว|ง|ม|ิ|ใ|ห|้|ค|ว|า|ม|ร|ุ|น|แ|ร|ง|ร|ะ|บ|า|ด|ไ|ป|ย|ั|ง|ส|่|ว|น|อ|ื|่|น|ๆ| |ข|อ|ง|ส|ั|ง|ค|ม|ไ|ด|้| |ฉ|ะ|น|ั|้|น|ส|ั|ง|ค|ม|จ|ึ\n",
      "Loss:  20.1706\r"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "printEvery = 1000\n",
    "i=0\n",
    "j=0\n",
    "for i in range(epochs):\n",
    "    print(\"Starting epochs %d...\"%i)\n",
    "    start_time = time.time()\n",
    "    shuffled_indexed = np.random.permutation(len(X))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    for sentence, tags in zip(X[shuffled_indexed],Y[shuffled_indexed]):\n",
    "        piped = preprocess.create_pipe(sentence,tags)\n",
    "        if(piped):\n",
    "            j+=1\n",
    "            if(j%printEvery==0): print(piped)\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Variables of word indices.\n",
    "            sentence_in = prepare_sequence(sentence)\n",
    "            #targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "            targets = Variable(torch.FloatTensor([int(i) for i in tags])).view(-1,1)\n",
    "\n",
    "\n",
    "            pred_string = \"\".join([str(int(i[0])) for i in tag_scores.data.numpy()])\n",
    "            if(j%printEvery==0): \n",
    "                print(\"Pred: \",end='')\n",
    "                print(preprocess.create_pipe(sentence,pred_string))\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            print(\"Loss: \",loss.data.numpy()[0],end='\\r')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(\"Skipping\")\n",
    "        \n",
    "    print(\"Epochs: %d\"%i)\n",
    "    print(\"Done!\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Loss: %f\"%(loss.data.numpy()[0]))\n",
    "    train_loss.append(loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
