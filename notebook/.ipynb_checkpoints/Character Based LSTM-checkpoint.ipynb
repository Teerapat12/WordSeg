{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "import sys\n",
    "sys.path.insert(0, '../script/')\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import preprocess\n",
    "X,Y = preprocess.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished cleaning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 22\n",
       " 19\n",
       " 41\n",
       " 44\n",
       " 25\n",
       "[torch.LongTensor of size 5]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Finished cleaning\")\n",
    "def charToNumber(c):\n",
    "    n = ord(c)\n",
    "    if(n>=3585 and n<=3673):\n",
    "        return n-3585\n",
    "    else:\n",
    "        return 89 #For Other\n",
    "\n",
    "def prepare_sequence(sentence):\n",
    "    idxs = [charToNumber(c) for c in sentence]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return Variable(tensor)\n",
    "prepare_sequence(\"ทดสอบ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initiate parameters\n",
    "emb_size = 32\n",
    "nhidden = 300\n",
    "nlayers = 5\n",
    "dropout = 0.5\n",
    "rnn_type = 'LSTM'\n",
    "n_char = 90\n",
    "clip = 0.25\n",
    "nout = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers,char_size, tagset_size):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(char_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim//2,num_layers = n_layers,bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(self.n_layers*2, 1, self.hidden_dim//2)),\n",
    "                autograd.Variable(torch.zeros(self.n_layers*2, 1, self.hidden_dim//2)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.softmax(tag_space)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTMTagger(emb_size,nhidden,nlayers,n_char, nout)\n",
    "loss_function = nn.BCELoss()\n",
    "#loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epochs 0...\n",
      "(21589,)\n",
      "(21589,)\n",
      "าง|ใน|ปี| |พ.ศ.| |2470| |พระอาจารย์มั่น| |เดินทาง|เข้า|มา|ใน|เขต|อำเภออำนาจเจริญ| |จังหวัดอุบลราชธานี| |(|ขณะ|นั้น|)| |ได้|มี\n",
      "Pred: |า|ง|ใ|น|ป|ี| |พ|.|ศ|.| |2|4|7|0| |พ|ร|ะ|อ|า|จ|า|ร|ย|์|ม|ั|่|น| |เ|ด|ิ|น|ท|า|ง|เ|ข|้|า|ม|า|ใ|น|เ|ข|ต|อ|ำ|เ|ภ|อ|อ|ำ|น|า|จ|เ|จ|ร|ิ|ญ| |จ|ั|ง|ห|ว|ั|ด|อ|ุ|บ|ล|ร|า|ช|ธ|า|น|ี| |(|ข|ณ|ะ|น|ั|้|น|)| |ไ|ด|้|ม|ี\n",
      "ือ|สมภาร|ของ|วัด|หนึ่ง|ๆ| |?|สอง|รูป|จาก|ทาง|เหนือ|ของ|ไทย| |และ|หนึ่ง|รูป|จาก|กรุงเทพฯ| |กำลัง|ก้ม|คาราวะ|จรด|พื้น|ต่อ|ผู้|หญิง|ท\n",
      "Pred: |ื|อ|ส|ม|ภ|า|ร|ข|อ|ง|ว|ั|ด|ห|น|ึ|่|ง|ๆ| |?|ส|อ|ง|ร|ู|ป|จ|า|ก|ท|า|ง|เ|ห|น|ื|อ|ข|อ|ง|ไ|ท|ย| |แ|ล|ะ|ห|น|ึ|่|ง|ร|ู|ป|จ|า|ก|ก|ร|ุ|ง|เ|ท|พ|ฯ| |ก|ำ|ล|ั|ง|ก|้|ม|ค|า|ร|า|ว|ะ|จ|ร|ด|พ|ื|้|น|ต|่|อ|ผ|ู|้|ห|ญ|ิ|ง|ท\n",
      "ัน|)| |...| |มัน|ไม่|เหมือน|การ|จ้าง| |...| |การ|จ้าง| |เงิน|คือ|สิ่ง|สำคัญ| |คุณ|จะ|ทำ|งาน|ใน|ช่วง|เวลา|ที่|แน่นอน| |...| |แต่|ทว่า|กา\n",
      "Pred: |ั|น|)| |.|.|.| |ม|ั|น|ไ|ม|่|เ|ห|ม|ื|อ|น|ก|า|ร|จ|้|า|ง| |.|.|.| |ก|า|ร|จ|้|า|ง| |เ|ง|ิ|น|ค|ื|อ|ส|ิ|่|ง|ส|ำ|ค|ั|ญ| |ค|ุ|ณ|จ|ะ|ท|ำ|ง|า|น|ใ|น|ช|่|ว|ง|เ|ว|ล|า|ท|ี|่|แ|น|่|น|อ|น| |.|.|.| |แ|ต|่|ท|ว|่|า|ก|า\n",
      "72ss:  20.9996\n",
      "73\n",
      "านเล็กในทุ่งกว้าง, สุคนธรส (แปล) (กรุงเทพฯ: แพรวเยาวชน, 2548), หน้า 156.\n",
      "0010001010001000011100000011100111100000001110001000001110001111000110011\n",
      "Skipping\n",
      "าง|ของ| |McUniversity| |กระบวนการ|แม็กโดนัลดานุวัตร| |(|McDonaldization|)| |มิ|ได้|ก่อเกิด|ใน|อุตสาหกรรม|อาหาร|และ|เครื\n",
      "Pred: |า|ง|ข|อ|ง| |M|c|U|n|i|v|e|r|s|i|t|y| |ก|ร|ะ|บ|ว|น|ก|า|ร|แ|ม|็|ก|โ|ด|น|ั|ล|ด|า|น|ุ|ว|ั|ต|ร| |(|M|c|D|o|n|a|l|d|i|z|a|t|i|o|n|)| |ม|ิ|ไ|ด|้|ก|่|อ|เ|ก|ิ|ด|ใ|น|อ|ุ|ต|ส|า|ห|ก|ร|ร|ม|อ|า|ห|า|ร|แ|ล|ะ|เ|ค|ร|ื\n",
      "50ss:  21.5522\n",
      "51\n",
      "114. (อ่านเพิ่มเติมเกี่ยวกับรื่องนี้ที่ภาคผนวก ข.)\n",
      "100111100010000000010000010010000100100100100011111\n",
      "Skipping\n",
      "้า|ไป|มี|อิทธิพล|กรณี|บทบาท|ของ|เสียง|ข้าง|มาก|ใน|วุฒิสภา|,| |ใน|ศาลรัฐธรรมนูญ|,| |ใน| |ปปช.| |เป็นต้น|.| |การ|แก้ไข|ปัญหา|ภาค|ใต\n",
      "Pred: |้|า|ไ|ป|ม|ี|อ|ิ|ท|ธ|ิ|พ|ล|ก|ร|ณ|ี|บ|ท|บ|า|ท|ข|อ|ง|เ|ส|ี|ย|ง|ข|้|า|ง|ม|า|ก|ใ|น|ว|ุ|ฒ|ิ|ส|ภ|า|,| |ใ|น|ศ|า|ล|ร|ั|ฐ|ธ|ร|ร|ม|น|ู|ญ|,| |ใ|น| |ป|ป|ช|.| |เ|ป|็|น|ต|้|น|.| |ก|า|ร|แ|ก|้|ไ|ข|ป|ั|ญ|ห|า|ภ|า|ค|ใ|ต\n",
      "าตานี|อ่อน|กำลัง|ลง|อย่าง|มาก| |ดู|เพิ่มเติม|ได้|จาก| |Uthai Dulyakasem|,| |'|Muslim| |Malays| |in| |southern| |Thailand|:| |Fa\n",
      "Pred: |า|ต|า|น|ี|อ|่|อ|น|ก|ำ|ล|ั|ง|ล|ง|อ|ย|่|า|ง|ม|า|ก| |ด|ู|เ|พ|ิ|่|ม|เ|ต|ิ|ม|ไ|ด|้|จ|า|ก| |U|t|h|a|i| |D|u|l|y|a|k|a|s|e|m|,| |'|M|u|s|l|i|m| |M|a|l|a|y|s| |i|n| |s|o|u|t|h|e|r|n| |T|h|a|i|l|a|n|d|:| |F|a\n",
      "ันธ์|ทาง|กฎหมาย| |งาน|ชิ้น|นี้|มี|ฐานะ|เป็น|เพียง|บท|นำ|ทาง|ความ|คิด| |เพื่อ|ปู|ทาง|ไป|สู่|การ|ทำ|ความ|เข้าใจ|ความ|สัมพันธ์|ระหว\n",
      "Pred: |ั|น|ธ|์|ท|า|ง|ก|ฎ|ห|ม|า|ย| |ง|า|น|ช|ิ|้|น|น|ี|้|ม|ี|ฐ|า|น|ะ|เ|ป|็|น|เ|พ|ี|ย|ง|บ|ท|น|ำ|ท|า|ง|ค|ว|า|ม|ค|ิ|ด| |เ|พ|ื|่|อ|ป|ู|ท|า|ง|ไ|ป|ส|ู|่|ก|า|ร|ท|ำ|ค|ว|า|ม|เ|ข|้|า|ใ|จ|ค|ว|า|ม|ส|ั|ม|พ|ั|น|ธ|์|ร|ะ|ห|ว\n",
      "อ|โรคภัย|ใหม่|ๆ| |ที่|คุกคาม|ชีวิต|ผู้คน|ถูก|วิเคราะห์|ว่า| |มี|สาเหตุ|มา|จาก|อาหารการกิน| |(|diet|-|related| |conditions| |of\n",
      "Pred: |อ|โ|ร|ค|ภ|ั|ย|ใ|ห|ม|่|ๆ| |ท|ี|่|ค|ุ|ก|ค|า|ม|ช|ี|ว|ิ|ต|ผ|ู|้|ค|น|ถ|ู|ก|ว|ิ|เ|ค|ร|า|ะ|ห|์|ว|่|า| |ม|ี|ส|า|เ|ห|ต|ุ|ม|า|จ|า|ก|อ|า|ห|า|ร|ก|า|ร|ก|ิ|น| |(|d|i|e|t|-|r|e|l|a|t|e|d| |c|o|n|d|i|t|i|o|n|s| |o|f\n",
      "42ss:  19.3417\n",
      "43\n",
      "ู้เขียนเชื่อมั่นว่า พิภพและพวกก็คิดเช่นนี้\n",
      "0010000100000000100110001001001010010001001\n",
      "Skipping\n",
      "้านเมือง|ตน|อย่าง|ทั่วถึง| |แต่|ใน|ปัจจุบัน|ก็|ยังคง|ปรากฏ|เป็น|ค่า|นิยม|ทาง|ศาสนา|พราหมณ์|ที่|เกี่ยวข้อง|กับ|การ|ปฏิบัติ\n",
      "Pred: |้|า|น|เ|ม|ื|อ|ง|ต|น|อ|ย|่|า|ง|ท|ั|่|ว|ถ|ึ|ง| |แ|ต|่|ใ|น|ป|ั|จ|จ|ุ|บ|ั|น|ก|็|ย|ั|ง|ค|ง|ป|ร|า|ก|ฏ|เ|ป|็|น|ค|่|า|น|ิ|ย|ม|ท|า|ง|ศ|า|ส|น|า|พ|ร|า|ห|ม|ณ|์|ท|ี|่|เ|ก|ี|่|ย|ว|ข|้|อ|ง|ก|ั|บ|ก|า|ร|ป|ฏ|ิ|บ|ั|ต|ิ\n",
      "น|เหตุ|ของ|ความ|ปั่นป่วน|อนาธิปัตย์| |และ|ภาวะ|แห่ง|ความ|ไร้|ระเบียบ|และ|กฎเกณฑ์|ของ|สังคม|จอนห์ ฟิสเก้| |(|John Fiske|)| \n",
      "Pred: |น|เ|ห|ต|ุ|ข|อ|ง|ค|ว|า|ม|ป|ั|่|น|ป|่|ว|น|อ|น|า|ธ|ิ|ป|ั|ต|ย|์| |แ|ล|ะ|ภ|า|ว|ะ|แ|ห|่|ง|ค|ว|า|ม|ไ|ร|้|ร|ะ|เ|บ|ี|ย|บ|แ|ล|ะ|ก|ฎ|เ|ก|ณ|ฑ|์|ข|อ|ง|ส|ั|ง|ค|ม|จ|อ|น|ห|์| |ฟ|ิ|ส|เ|ก|้| |(|J|o|h|n| |F|i|s|k|e|)| \n",
      "การณ์|ใน|วัน|ที่| |6| |ตุลาคม| |2519| |ไม่|เปิด|โอกาส|ให้|เขา|มี|ทาง|เลือก|ที่| |3| |เสีย|แล้ว| |ถ้า|ไม่|ทำ|ตัว|สงบเสงี่ยม|คล้อยตาม\n",
      "Pred: |ก|า|ร|ณ|์|ใ|น|ว|ั|น|ท|ี|่| |6| |ต|ุ|ล|า|ค|ม| |2|5|1|9| |ไ|ม|่|เ|ป|ิ|ด|โ|อ|ก|า|ส|ใ|ห|้|เ|ข|า|ม|ี|ท|า|ง|เ|ล|ื|อ|ก|ท|ี|่| |3| |เ|ส|ี|ย|แ|ล|้|ว| |ถ|้|า|ไ|ม|่|ท|ำ|ต|ั|ว|ส|ง|บ|เ|ส|ง|ี|่|ย|ม|ค|ล|้|อ|ย|ต|า|ม\n",
      "26ss:  22.3811\n",
      "27\n",
      "line.com/movie/drama/moon/\n",
      "000000000000000000000000001\n",
      "Skipping\n",
      "Loss:  20.7233\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1ccfdfbf46e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/time/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/time/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "printEvery = 50\n",
    "i=0\n",
    "j=0\n",
    "for i in range(epochs):\n",
    "    print(\"Starting epochs %d...\"%i)\n",
    "    start_time = time.time()\n",
    "    shuffled_indexed = np.random.permutation(len(X))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    for sentence, tags in zip(X[shuffled_indexed],Y[shuffled_indexed]):\n",
    "        piped = preprocess.create_pipe(sentence,tags)\n",
    "        if(piped):\n",
    "            j+=1\n",
    "            if(j%printEvery==0): print(piped)\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Variables of word indices.\n",
    "            sentence_in = prepare_sequence(sentence)\n",
    "            #targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "            targets = Variable(torch.FloatTensor([int(i) for i in tags])).view(-1,1)\n",
    "\n",
    "\n",
    "            pred_string = \"\".join([str(int(i[0])) for i in tag_scores.data.numpy()])\n",
    "            if(j%printEvery==0): \n",
    "                print(\"Pred: \",end='')\n",
    "                print(preprocess.create_pipe(sentence,pred_string))\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            print(\"Loss: \",loss.data.numpy()[0],end='\\r')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(\"Skipping\")\n",
    "        \n",
    "    print(\"Epochs: %d\"%i)\n",
    "    print(\"Done!\")\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Loss: %f\"%(loss.data.numpy()[0]))\n",
    "    train_loss.append(loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
